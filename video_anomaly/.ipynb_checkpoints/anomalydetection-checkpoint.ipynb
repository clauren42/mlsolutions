{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Anomaly detection\n",
    "\n",
    "The automation of detecting anomalous events in videos is a challenging problem that currently attracts a lot of attention by researchers, but also has broad applications across industry verticals.  \n",
    "\n",
    "The approach involves training deep neural networks to develop an in-depth understanding of the physical and causal rules in the observed scenes. The model effectively learns to predict future frames in the video. By calculating the error in this prediction, it is then possible to detect if something unusual, an anomalous event, occurred, if there is a large prediction error.  \n",
    "\n",
    "The approach can be used both in a supervised and unsupervised fashion, thus enabling the detection of pre-defined anomalies, but also of anomalous events that have never occurred in the past. \n",
    "\n",
    "# Learning Goals\n",
    "\n",
    "You will learn:\n",
    "1. How to adapt an existing neural network architecture to your use-case.\n",
    "1. How to prepare video data for deep learning. \n",
    "1. How to perform hyperparameter tuning with [HyperDrive](https://azure.microsoft.com/en-us/blog/experimentation-using-azure-machine-learning/) to improve the performance of you model.\n",
    "1. How to deploy a deep neural network as a webservice for video processing. \n",
    "1. How to post-process the output of a Keras model for secondary tasks (here, anomaly detection)\n",
    "2. How to define a build pipeline for DevOps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"800\" controls>\n",
       "  <source src=\"anomaly.mp4\" type=\"video/mp4\">\n",
       "</video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<video width=\"800\" controls>\n",
    "  <source src=\"anomaly.mp4\" type=\"video/mp4\">\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Azure Machine Learning workspace and prepare compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config(path=config_json, auth=svc_pr)\n",
    "\n",
    "\n",
    "# choose a name for your cluster\n",
    "gpu_compute_name = config['gpu_compute']\n",
    "\n",
    "try:\n",
    "    gpu_compute_target = AmlCompute(workspace=ws, name=gpu_compute_name)\n",
    "    print(\"found existing compute target: %s\" % gpu_compute_name)\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                                max_nodes=5,\n",
    "                                                                idle_seconds_before_scaledown=1800)\n",
    "\n",
    "    # create the cluster\n",
    "    gpu_compute_target = ComputeTarget.create(ws, gpu_compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it uses the scale settings for the cluster\n",
    "    gpu_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = TensorFlow(source_directory=script_folder,\n",
    "                compute_target=gpu_compute_target,\n",
    "                pip_packages=['keras==2.0.8', 'theano', 'tensorflow==1.8.0', 'tensorflow-gpu==1.8.0', 'matplotlib', 'horovod', 'hickle'],\n",
    "                entry_script='train.py', \n",
    "                use_gpu=True,\n",
    "                node_count=1,\n",
    "                script_params={\"--remote_execution\": None, \"--data-folder\": \"https://coursematerial.blob.core.windows.net/ucsd-ad/\"}\n",
    "                )\n",
    "                \n",
    "experiment_name = \"prednet_train\"\n",
    "\n",
    "exp = Experiment(ws, experiment_name)\n",
    "\n",
    "run = exp.submit(est)\n",
    "\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatically tune hyperparameters with Azure Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        '--batch_size': choice(2, 4, 8, 16),\n",
    "        '--filter_sizes': choice(\"3 3 3\", \"4 4 4\", \"5 5 5\"),\n",
    "        '--stack_sizes': choice(\"48 96 192\", \"36 72 144\", \"12 24 48\"),\n",
    "        '--learning_rate': loguniform(-6, -1),\n",
    "        '--lr_decay': loguniform(-9, -1)\n",
    "    }\n",
    ")\n",
    "\n",
    "policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)#, delay_evaluation=20)\n",
    "\n",
    "hdc = HyperDriveRunConfig(estimator=est, \n",
    "                          hyperparameter_sampling=ps, \n",
    "                          policy=policy, \n",
    "                          primary_metric_name='val_loss', \n",
    "                          primary_metric_goal=PrimaryMetricGoal.MINIMIZE, \n",
    "                          max_total_runs=5,\n",
    "                          max_concurrent_runs=50)\n",
    "\n",
    "hdr = exp.submit(config=hdc)\n",
    "\n",
    "hdr.wait_for_completion(show_output=True)\n",
    "\n",
    "best_run = hdr.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "print(best_run)\n",
    "\n",
    "# Writing the run id to /aml_config/run_id.json for use by a DevOps pipeline.\n",
    "run_id = {}\n",
    "run_id['run_id'] = best_run.id\n",
    "run_id['experiment_name'] = best_run.experiment.name\n",
    "\n",
    "# save run info \n",
    "os.makedirs('aml_config', exist_ok = True)\n",
    "with open('aml_config/run_id.json', 'w') as outfile:\n",
    "    json.dump(run_id, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the model\n",
    "if best_run_id:\n",
    "    tags = {}\n",
    "    tags['run_id'] = best_run_id\n",
    "    tags['val_loss'] = metrics[best_run_id]['val_loss'][-1]\n",
    "    model = best_run.register_model(model_name=experiment_name, \n",
    "                                    model_path='outputs',\n",
    "                                    tags=tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Azure Container Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.image import Image\n",
    "from azureml.core import Workspace\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# Get workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get the Image to deploy details\n",
    "try:\n",
    "    with open(\"aml_config/image.json\") as f:\n",
    "        config = json.load(f)\n",
    "except:\n",
    "    print('No new model, thus no deployment on ACI')\n",
    "    sys.exit(0)\n",
    "\n",
    "image_name = config['image_name']\n",
    "image_version = config['image_version']\n",
    "\n",
    "images = Image.list(workspace=ws)\n",
    "image, = (m for m in images if m.version==image_version and m.name == image_name)\n",
    "print('From image.json, Image used to deploy webservice on ACI: {}\\nImage Version: {}\\nImage Location = {}'.format(image.name, image.version, image.image_location))\n",
    "\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               auth_enabled=True, # this flag generates API keys to secure access\n",
    "                                               memory_gb=1, \n",
    "                                               tags={'name':'prednet', 'framework': 'Keras'},\n",
    "                                               description='Prednet')\n",
    "\n",
    "\n",
    "aci_service_name = image_name\n",
    "print(aci_service_name)\n",
    "\n",
    "service = Webservice.deploy_from_image(deployment_config = aciconfig,\n",
    "                                           image = image,\n",
    "                                           name = aci_service_name,\n",
    "                                           workspace = ws)\n",
    "service.wait_for_deployment(True)\n",
    "\n",
    "print('Deployed ACI Webservice: {} \\nWebservice Uri: {}'.format(service.name, service.scoring_uri))\n",
    "\n",
    "#service=Webservice(name ='aciws0622', workspace =ws)\n",
    "# Writing the ACI details to /aml_config/aci_webservice.json\n",
    "aci_webservice = {}\n",
    "aci_webservice['aci_name'] = service.name\n",
    "aci_webservice['aci_url'] = service.scoring_uri\n",
    "with open('aml_config/aci_webservice.json', 'w') as outfile:\n",
    "  json.dump(aci_webservice,outfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate repeatable process with Azure Machine Learning pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import azureml\n",
    "import shutil\n",
    "import socket\n",
    "from azureml.core import Workspace, Run, Experiment, Datastore\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, BanditPolicy, HyperDriveRunConfig, PrimaryMetricGoal\n",
    "from azureml.pipeline.steps import HyperDriveStep\n",
    "from azureml.pipeline.core import PublishedPipeline\n",
    "from azureml.train.hyperdrive import choice, loguniform\n",
    "from azureml.train.dnn import TensorFlow\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from azureml.core.runconfig import DEFAULT_GPU_IMAGE, DEFAULT_CPU_IMAGE\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "def build_pipeline(dataset, ws, config):\n",
    "    print(\"building pipeline for dataset %s in workspace %s\" % (dataset, ws.name))\n",
    "\n",
    "    hostname = socket.gethostname()\n",
    "    if hostname == 'wopauliNC6':\n",
    "        base_dir = '.'\n",
    "    else:\n",
    "        base_dir = '.'\n",
    "        \n",
    "    def_blob_store = ws.get_default_datastore()\n",
    "\n",
    "    # folder for scripts that need to be uploaded to Aml compute target\n",
    "    script_folder = './scripts'\n",
    "    os.makedirs(script_folder, exist_ok=True)\n",
    "    \n",
    "    shutil.copy(os.path.join(base_dir, 'video_decoding.py'), script_folder)\n",
    "    shutil.copy(os.path.join(base_dir, 'pipelines_submit.py'), script_folder)\n",
    "    shutil.copy(os.path.join(base_dir, 'pipelines_build.py'), script_folder)\n",
    "    shutil.copy(os.path.join(base_dir, 'train.py'), script_folder)\n",
    "    shutil.copy(os.path.join(base_dir, 'data_utils.py'), script_folder)\n",
    "    shutil.copy(os.path.join(base_dir, 'prednet.py'), script_folder)\n",
    "    shutil.copy(os.path.join(base_dir, 'keras_utils.py'), script_folder)\n",
    "    shutil.copy(os.path.join(base_dir, 'data_preparation.py'), script_folder)\n",
    "    shutil.copy(os.path.join(base_dir, 'model_registration.py'), script_folder)\n",
    "    shutil.copy(os.path.join(base_dir, 'config.json'), script_folder)\n",
    "\n",
    "    cpu_compute_name = config['cpu_compute']\n",
    "    try:\n",
    "        cpu_compute_target = AmlCompute(ws, cpu_compute_name)\n",
    "        print(\"found existing compute target: %s\" % cpu_compute_name)\n",
    "    except ComputeTargetException:\n",
    "        print(\"creating new compute target\")\n",
    "        \n",
    "        provisioning_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', \n",
    "                                                                    max_nodes=4,\n",
    "                                                                    idle_seconds_before_scaledown=1800)    \n",
    "        cpu_compute_target = ComputeTarget.create(ws, cpu_compute_name, provisioning_config)\n",
    "        cpu_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "        \n",
    "    # use get_status() to get a detailed status for the current cluster. \n",
    "    print(cpu_compute_target.get_status().serialize())\n",
    "\n",
    "    # choose a name for your cluster\n",
    "    gpu_compute_name = config['gpu_compute']\n",
    "\n",
    "    try:\n",
    "        gpu_compute_target = AmlCompute(workspace=ws, name=gpu_compute_name)\n",
    "        print(\"found existing compute target: %s\" % gpu_compute_name)\n",
    "    except ComputeTargetException:\n",
    "        print('Creating a new compute target...')\n",
    "        provisioning_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', \n",
    "                                                                    max_nodes=5,\n",
    "                                                                    idle_seconds_before_scaledown=1800)\n",
    "\n",
    "        # create the cluster\n",
    "        gpu_compute_target = ComputeTarget.create(ws, gpu_compute_name, provisioning_config)\n",
    "\n",
    "        # can poll for a minimum number of nodes and for a specific timeout. \n",
    "        # if no min node count is provided it uses the scale settings for the cluster\n",
    "        gpu_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # use get_status() to get a detailed status for the current cluster. \n",
    "    print(gpu_compute_target.get_status().serialize())\n",
    "\n",
    "    # conda dependencies for compute targets\n",
    "    cpu_cd = CondaDependencies.create(conda_packages=[\"py-opencv=3.4.2\"], pip_packages=[\"azure-storage-blob==1.5.0\", \"hickle==3.4.3\", \"requests==2.21.0\", \"sklearn\", \"pandas==0.24.2\", \"azureml-sdk==1.0.21\", \"numpy==1.16.2\", \"pillow==6.0.0\"])\n",
    "    gpu_cd = CondaDependencies.create(pip_packages=[\"keras==2.0.8\", \"theano==1.0.4\", \"tensorflow==1.8.0\", \"tensorflow-gpu==1.8.0\", \"hickle==3.4.3\", \"matplotlib==3.0.3\", \"seaborn==0.9.0\", \"requests==2.21.0\", \"bs4==0.0.1\", \"imageio==2.5.0\", \"sklearn\", \"pandas==0.24.2\", \"azureml-sdk==1.0.21\", \"numpy==1.16.2\"])\n",
    "\n",
    "    # Runconfigs\n",
    "    cpu_compute_run_config = RunConfiguration(conda_dependencies=cpu_cd)\n",
    "    cpu_compute_run_config.environment.docker.enabled = True\n",
    "    cpu_compute_run_config.environment.docker.gpu_support = False\n",
    "    cpu_compute_run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "    cpu_compute_run_config.environment.spark.precache_packages = False\n",
    "\n",
    "    gpu_compute_run_config = RunConfiguration(conda_dependencies=gpu_cd)\n",
    "    gpu_compute_run_config.environment.docker.enabled = True\n",
    "    gpu_compute_run_config.environment.docker.gpu_support = True\n",
    "    gpu_compute_run_config.environment.docker.base_image = DEFAULT_GPU_IMAGE\n",
    "    gpu_compute_run_config.environment.spark.precache_packages = False\n",
    "\n",
    "\n",
    "    print(\"PipelineData object created\")\n",
    "\n",
    "    video_data = DataReference(\n",
    "        datastore=def_blob_store,\n",
    "        data_reference_name=\"video_data\",\n",
    "        path_on_datastore=os.path.join(\"prednet\", \"data\", \"video\", dataset))\n",
    "        \n",
    "    # Naming the intermediate data as processed_data1 and assigning it to the variable processed_data1.\n",
    "    raw_data = PipelineData(\"raw_video_fames\", datastore=def_blob_store)\n",
    "    preprocessed_data = PipelineData(\"preprocessed_video_frames\", datastore=def_blob_store)\n",
    "    data_metrics = PipelineData(\"data_metrics\", datastore=def_blob_store)\n",
    "    data_output = PipelineData(\"output_data\", datastore=def_blob_store)\n",
    "\n",
    "\n",
    "    print(\"DataReference object created\")\n",
    "\n",
    "    # prepare dataset for training/testing prednet\n",
    "    video_decoding = PythonScriptStep(\n",
    "        name='decode_videos',\n",
    "        script_name=\"video_decoding.py\", \n",
    "        arguments=[\"--input_data\", video_data, \"--output_data\", raw_data],\n",
    "        inputs=[video_data],\n",
    "        outputs=[raw_data],\n",
    "        compute_target=cpu_compute_target, \n",
    "        source_directory=script_folder,\n",
    "        runconfig=cpu_compute_run_config,\n",
    "        allow_reuse=True,\n",
    "        hash_paths=['.']\n",
    "    )\n",
    "    print(\"video_decode created\")\n",
    "\n",
    "    # prepare dataset for training/testing recurrent neural network\n",
    "    data_prep = PythonScriptStep(\n",
    "        name='prepare_data',\n",
    "        script_name=\"data_preparation.py\", \n",
    "        arguments=[\"--input_data\", raw_data, \"--output_data\", preprocessed_data],\n",
    "        inputs=[raw_data],\n",
    "        outputs=[preprocessed_data],\n",
    "        compute_target=cpu_compute_target, \n",
    "        source_directory=script_folder,\n",
    "        runconfig=cpu_compute_run_config,\n",
    "        allow_reuse=True,\n",
    "        hash_paths=['.']\n",
    "    )\n",
    "    data_prep.run_after(video_decoding)\n",
    "\n",
    "    print(\"data_prep created\")\n",
    "\n",
    "    est = TensorFlow(source_directory=script_folder,\n",
    "                    compute_target=gpu_compute_target,\n",
    "                    pip_packages=['keras==2.0.8', 'theano', 'tensorflow==1.8.0', 'tensorflow-gpu==1.8.0', 'matplotlib', 'horovod', 'hickle'],\n",
    "                    entry_script='train.py', \n",
    "                    use_gpu=True,\n",
    "                    node_count=1)\n",
    "\n",
    "\n",
    "    ps = RandomParameterSampling(\n",
    "        {\n",
    "            '--batch_size': choice(2, 4, 8, 16),\n",
    "            '--filter_sizes': choice(\"3, 3, 3\", \"4, 4, 4\", \"5, 5, 5\"),\n",
    "            '--stack_sizes': choice(\"48, 96, 192\", \"36, 72, 144\", \"12, 24, 48\"), #, \"48, 96\"),\n",
    "            '--learning_rate': loguniform(-6, -1),\n",
    "            '--lr_decay': loguniform(-9, -1),\n",
    "            '--freeze_layers': choice(\"0, 1, 2\", \"1, 2, 3\", \"0, 1\", \"1, 2\", \"2, 3\", \"0\", \"1\", \"2\", \"3\"),\n",
    "            '--transfer_learning': choice(\"True\", \"False\")\n",
    "        }\n",
    "    )\n",
    "\n",
    "    policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1, delay_evaluation=20)\n",
    "\n",
    "    hdc = HyperDriveRunConfig(estimator=est, \n",
    "                            hyperparameter_sampling=ps, \n",
    "                            policy=policy, \n",
    "                            primary_metric_name='val_loss', \n",
    "                            primary_metric_goal=PrimaryMetricGoal.MINIMIZE, \n",
    "                            max_total_runs=5, #100,\n",
    "                            max_concurrent_runs=5, #10,\n",
    "                            max_duration_minutes=60*6\n",
    "                            )\n",
    "\n",
    "    hd_step = HyperDriveStep(\n",
    "        name=\"train_w_hyperdrive\",\n",
    "        hyperdrive_run_config=hdc,\n",
    "        estimator_entry_script_arguments=['--data-folder', preprocessed_data, '--remote_execution'],\n",
    "        inputs=[preprocessed_data],\n",
    "        metrics_output = data_metrics,\n",
    "        allow_reuse=True\n",
    "    )\n",
    "    hd_step.run_after(data_prep)\n",
    "\n",
    "    registration_step = PythonScriptStep(\n",
    "        name='register_model',\n",
    "        script_name='model_registration.py',\n",
    "        arguments=['--input_dir', data_metrics, '--output_dir', data_output],\n",
    "        compute_target=gpu_compute_target,\n",
    "        inputs=[data_metrics],\n",
    "        outputs=[data_output],\n",
    "        source_directory=script_folder,\n",
    "        allow_reuse=True,\n",
    "        hash_paths=['.']\n",
    "    )\n",
    "    registration_step.run_after(hd_step)\n",
    "\n",
    "    pipeline = Pipeline(workspace=ws, steps=[video_decoding, data_prep, hd_step, registration_step])\n",
    "    print (\"Pipeline is built\")\n",
    "\n",
    "    pipeline.validate()\n",
    "    print(\"Simple validation complete\") \n",
    "\n",
    "    pipeline_name = 'prednet_' + dataset\n",
    "    pipeline.publish(name=pipeline_name)\n",
    "    \n",
    "    return pipeline_name\n",
    "\n",
    "\n",
    "def_blob_store  = ws.get_default_datastore()\n",
    "\n",
    "print(\"Blobstore's name: {}\".format(def_blob_store.name))\n",
    "\n",
    "# create a list of datasets stored in blob\n",
    "print(\"Checking for new datasets\")\n",
    "blob_service = BlockBlobService(def_blob_store.account_name, def_blob_store.account_key)\n",
    "generator = blob_service.list_blobs(def_blob_store.container_name, prefix=\"prednet/data/video\")\n",
    "datasets = []\n",
    "for blob in generator:\n",
    "    dataset = blob.name.split('/')[3]\n",
    "    if dataset not in datasets and dataset.startswith(\"UCSD\"):\n",
    "        datasets.append(dataset)\n",
    "        print(\"Found dataset:\", dataset)\n",
    "\n",
    "# Get all published pipeline objects in the workspace\n",
    "all_pub_pipelines = PublishedPipeline.get_all(ws)\n",
    "\n",
    "# Create a list of datasets for which we have (old) and don't have (new) a published pipeline\n",
    "old_datasets = []\n",
    "new_datasets = []\n",
    "for dataset in datasets:\n",
    "    for pub_pipeline in all_pub_pipelines:\n",
    "        if pub_pipeline.name.endswith(dataset):\n",
    "            old_datasets.append(dataset)\n",
    "    if not dataset in old_datasets:\n",
    "        new_datasets.append(dataset)\n",
    "\n",
    "for dataset in new_datasets:\n",
    "    build_pipeline(dataset, ws, config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:myenv]",
   "language": "python",
   "name": "conda-env-myenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
